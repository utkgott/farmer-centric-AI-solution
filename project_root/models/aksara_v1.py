# -*- coding: utf-8 -*-
"""aksara_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//huggingface.co/cropinailab/aksara_v1.ipynb
"""

!pip install -U transformers

"""## Local Inference on GPU
Model page: https://huggingface.co/cropinailab/aksara_v1

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/cropinailab/aksara_v1)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè

The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó
"""

from huggingface_hub import login
login(new_session=False)

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="cropinailab/aksara_v1")
messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe(messages)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("cropinailab/aksara_v1")
model = AutoModelForCausalLM.from_pretrained("cropinailab/aksara_v1")
messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(
	messages,
	add_generation_prompt=True,
	tokenize=True,
	return_dict=True,
	return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))