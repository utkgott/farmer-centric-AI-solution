# -*- coding: utf-8 -*-
"""Crop_disease_with_output(Don't hit RUN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pAY2FXLzVW1KHMNrJb-MPNnYWB7QIRSY
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("kamal01/top-agriculture-crop-disease")

print("Path to dataset files:", path)

!ls "/kaggle/input/top-agriculture-crop-disease/"

!ls "/root/.cache/kagglehub/datasets/kamal01/top-agriculture-crop-disease/versions/1/"

"""##SETUP"""

!pip install torchmetrics

import os, torch, shutil, numpy as np, json, timm, torchmetrics
from glob import glob
from PIL import Image
from torch.utils.data import random_split, Dataset, DataLoader
from torchvision import transforms as T
from tqdm import tqdm
from matplotlib import pyplot as plt
import cv2
import random
from datetime import datetime
import pandas as pd

torch.manual_seed(2024)
np.random.seed(2024)

# Dataset Class
class CustomDataset(Dataset):

    def __init__(self, root, transformations = None):

        self.transformations = transformations
        self.im_paths = sorted(glob(f"{root}/*/*"))

        if not self.im_paths:
            raise FileNotFoundError(f"No images found in the directory: {root}. Please check the root directory path and ensure it contains subdirectories with images.")

        self.cls_names, self.cls_counts, count, data_count = {}, {}, 0, 0
        for idx, im_path in enumerate(self.im_paths):
            class_name = self.get_class(im_path)
            if class_name not in self.cls_names:
                self.cls_names[class_name] = count
                self.cls_counts[class_name] = 1
                count += 1
            else:
                self.cls_counts[class_name] += 1

    def get_class(self, path):
        return os.path.dirname(path).split("/")[-1]

    def __len__(self):
        return len(self.im_paths)

    def __getitem__(self, idx):

        im_path = self.im_paths[idx]
        im = Image.open(im_path).convert("RGB")
        gt = self.cls_names[self.get_class(im_path)]

        if self.transformations is not None:
            im = self.transformations(im)

        return im, gt

def get_dls(root, transformations, bs, split = [0.9, 0.05, 0.05], ns = 4):

    ds = CustomDataset(root = root, transformations = transformations)

    total_len = len(ds)
    print(f"Total images found: {total_len}")

    tr_len = int(total_len * split[0])
    vl_len = int(total_len * split[1])
    ts_len = total_len - (tr_len + vl_len)

    print(f"Calculated lengths: Train={tr_len}, Validation={vl_len}, Test={ts_len}")


    if total_len == 0:
        raise ValueError("Dataset is empty. Check your root directory.")
    if tr_len == 0:
        raise ValueError("Training dataset split is 0. Adjust split percentages or check dataset size.")
    if vl_len == 0:
        raise ValueError("Validation dataset split is 0. Adjust split percentages or check dataset size.")
    if ts_len == 0:
        raise ValueError("Test dataset split is 0. Adjust split percentages or check dataset size.")

    tr_ds, vl_ds, ts_ds = random_split(
        dataset = ds,
        lengths = [tr_len, vl_len, ts_len],
        generator=torch.Generator().manual_seed(2024)
    )

    if len(tr_ds) == 0:
         raise ValueError("Training dataset split is empty after random_split. Adjust split percentages or check dataset size.")
    if len(vl_ds) == 0:
         raise ValueError("Validation dataset split is empty after random_split. Adjust split percentages or check dataset size.")
    if len(ts_ds) == 0:
         raise ValueError("Test dataset split is empty after random_split. Adjust split percentages or check dataset size.")


    tr_dl = DataLoader(tr_ds, batch_size = bs, shuffle = True, num_workers = ns)
    val_dl = DataLoader(vl_ds, batch_size = bs, shuffle = False, num_workers = ns)
    ts_dl = DataLoader(ts_ds, batch_size = 1, shuffle = False, num_workers = ns)

    return tr_dl, val_dl, ts_dl, ds.cls_names

# Data Loading
root = "/root/.cache/kagglehub/datasets/kamal01/top-agriculture-crop-disease/versions/1/Crop Diseases" # Replace with the actual path to your dataset
mean, std, im_size = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 224
tfs = T.Compose([
    T.Resize((im_size, im_size)),
    T.ToTensor(),
    T.Normalize(mean = mean, std = std)
])

tr_dl, val_dl, ts_dl, classes = get_dls(root = root, transformations = tfs, bs = 32)

print(f"Train batches: {len(tr_dl)}")
print(f"Validation batches: {len(val_dl)}")
print(f"Test batches: {len(ts_dl)}")
print(f"Number of classes: {len(classes)}")
print(f"Classes: {classes}")

"""DATA VIZUALIZATION"""

def tensor_2_im(t, t_type = "rgb"):

    gray_tfs = T.Compose([
        T.Normalize(mean = [0.], std = [1/0.5]),
        T.Normalize(mean = [-0.5], std = [1])
    ])
    rgb_tfs = T.Compose([
        T.Normalize(mean = [0., 0., 0.], std = [1/0.229, 1/0.224, 1/0.225]),
        T.Normalize(mean = [-0.485, -0.456, -0.406], std = [1., 1., 1.])
    ])

    invTrans = gray_tfs if t_type == "gray" else rgb_tfs

    if t_type == "gray":
        return (invTrans(t) * 255).detach().squeeze().cpu().permute(1,2,0).numpy().astype(np.uint8)
    else:
        return (invTrans(t) * 255).detach().cpu().permute(1,2,0).numpy().astype(np.uint8)

def visualize(data, n_ims, rows, cmap = None, cls_names = None):

    assert cmap in ["rgb", "gray"], "Specify if the image is grayscale or color!"
    if cmap == "rgb":
        cmap_val = "viridis"
    else:
        cmap_val = cmap

    plt.figure(figsize = (20, 10))
    indices = [random.randint(0, len(data) - 1) for _ in range(n_ims)]

    for idx, indeks in enumerate(indices):

        im, gt = data[indeks]
        plt.subplot(rows, n_ims // rows, idx + 1)

        if cmap == "gray":
            plt.imshow(tensor_2_im(im, cmap), cmap=cmap_val)
        else:
            plt.imshow(tensor_2_im(im))

        plt.axis('off')

        if cls_names is not None:
            plt.title(f"GT -> {cls_names[int(gt)]}")
        else:
            plt.title(f"GT -> {gt}")

# Visualize samples
print("Training samples:")
visualize(tr_dl.dataset, 20, 4, "rgb", list(classes.keys()))

print("Validation samples:")
visualize(val_dl.dataset, 20, 4, "rgb", list(classes.keys()))

print("Test samples:")
visualize(ts_dl.dataset, 20, 4, "rgb", list(classes.keys()))

visualize(val_dl.dataset, 20, 4, "rgb", list(classes.keys()))

visualize(ts_dl.dataset, 20, 4, "rgb", list(classes.keys()))

"""DATA ANALSYSIS"""

def data_analysis(root, transformations):

    ds = CustomDataset(root = root, transformations = transformations)
    cls_counts = ds.cls_counts
    width, text_width = 0.7, 0.05
    text_height = 2
    cls_names = list(cls_counts.keys())
    counts = list(cls_counts.values())

    fig, ax = plt.subplots(figsize = (20, 10))
    indices = np.arange(len(counts))

    ax.bar(indices, counts, width, color = "firebrick")
    ax.set_xlabel("Class Names", color = "red", fontsize=14)
    ax.set_xticklabels(cls_names, rotation = 60)
    ax.set(xticks = indices, xticklabels = cls_names)
    ax.set_ylabel("Data Counts", color = "red", fontsize=14)
    ax.set_title(f"Dataset Class Imbalance Analysis", fontsize=16)

    for i, v in enumerate(counts):
        ax.text(i - text_width, v + text_height, str(v), color = "royalblue", fontsize=10)

    plt.tight_layout()
    plt.show()

    # Print statistics
    print(f"\nDataset Statistics:")
    print(f"Total images: {sum(counts)}")
    print(f"Number of classes: {len(cls_names)}")
    print(f"Min images per class: {min(counts)}")
    print(f"Max images per class: {max(counts)}")
    print(f"Average images per class: {np.mean(counts):.2f}")

data_analysis(root = root, transformations = tfs)

"""### AI Model Train and Validation

##MODEL CONFIGURATION DIRECTORY
"""

# Model configurations
MODEL_CONFIGS = {
    # 'ViT': {
    #     'model_name': 'vit_base_patch16_224',
    #     'pretrained': True,
    #     'description': 'Vision Transformer Base'
    # },
    # 'ConvNeXtV2-Base': {
    #     'model_name': 'convnextv2_base.fcmae_ft_in22k_in1k',
    #     'pretrained': True,
    #     'description': 'ConvNeXt V2 Base'
    # },
    # 'ConvNeXtV2-Small': {
    #     'model_name': 'convnextv2_small.fcmae_ft_in22k_in1k',
    #     'pretrained': True,
    #     'description': 'ConvNeXt V2 Small'
    # },
    'EfficientNetV2-Small': {
        'model_name': 'tf_efficientnetv2_s',
        'pretrained': True,
        'description': 'EfficientNet V2 Small'
    }
}

# Display available models
print("Models to be compared:")
for key, value in MODEL_CONFIGS.items():
    print(f"{key}: {value['description']} ({value['model_name']})")

"""##TRAINING AND SELF EVALUOTION"""

class ModelTrainer:

    def __init__(self, model_name, model, device, num_classes, save_dir="saved_models"):
        self.model_name = model_name
        self.model = model.to(device)
        self.device = device
        self.num_classes = num_classes
        self.save_dir = save_dir

        # Create save directory
        os.makedirs(save_dir, exist_ok=True)

        # Loss and optimizer
        self.loss_fn = torch.nn.CrossEntropyLoss()
        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=3e-4)

        # Metrics
        self.f1_score = torchmetrics.F1Score(
            task="multiclass",
            num_classes=num_classes
        ).to(device)

        # History
        self.history = {
            'train_loss': [], 'train_acc': [], 'train_f1': [],
            'val_loss': [], 'val_acc': [], 'val_f1': []
        }

    def to_device(self, batch):
        return batch[0].to(self.device), batch[1].to(self.device)

    def calculate_metrics(self, ims, gts):
        preds = self.model(ims)
        loss = self.loss_fn(preds, gts)
        acc = (torch.argmax(preds, dim=1) == gts).sum().item()
        f1 = self.f1_score(preds, gts)
        return loss, acc, f1

    def train_epoch(self, train_loader):
        self.model.train()
        epoch_loss, epoch_acc, epoch_f1 = 0, 0, 0

        for batch in tqdm(train_loader, desc=f"Training {self.model_name}"):
            ims, gts = self.to_device(batch)

            loss, acc, f1 = self.calculate_metrics(ims, gts)

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            epoch_loss += loss.item()
            epoch_acc += acc
            epoch_f1 += f1.item()

        return (
            epoch_loss / len(train_loader),
            epoch_acc / len(train_loader.dataset),
            epoch_f1 / len(train_loader)
        )

    def validate_epoch(self, val_loader):
        self.model.eval()
        epoch_loss, epoch_acc, epoch_f1 = 0, 0, 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Validating {self.model_name}"):
                ims, gts = self.to_device(batch)

                loss, acc, f1 = self.calculate_metrics(ims, gts)

                epoch_loss += loss.item()
                epoch_acc += acc
                epoch_f1 += f1.item()

        return (
            epoch_loss / len(val_loader),
            epoch_acc / len(val_loader.dataset),
            epoch_f1 / len(val_loader)
        )

    def train(self, train_loader, val_loader, epochs=10, patience=5, threshold=0.01):

        best_loss = float('inf')
        not_improved = 0

        print(f"\n{'='*60}")
        print(f"Training {self.model_name}")
        print(f"{'='*60}\n")

        for epoch in range(epochs):

            # Training
            train_loss, train_acc, train_f1 = self.train_epoch(train_loader)
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['train_f1'].append(train_f1)

            print(f"\nEpoch {epoch + 1}/{epochs}")
            print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}")

            # Validation
            val_loss, val_acc, val_f1 = self.validate_epoch(val_loader)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            self.history['val_f1'].append(val_f1)

            print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")

            # Save best model
            if val_loss < (best_loss - threshold):
                best_loss = val_loss
                not_improved = 0
                model_path = f"{self.save_dir}/{self.model_name}_best_model.pth"
                torch.save(self.model.state_dict(), model_path)
                print(f"✓ Model saved to {model_path}")
            else:
                not_improved += 1
                print(f"✗ No improvement for {not_improved} epoch(s)")

                if not_improved >= patience:
                    print(f"\nEarly stopping triggered after {epoch + 1} epochs")
                    break

        # Save history
        history_path = f"{self.save_dir}/{self.model_name}_history.json"
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=4)

        return self.history

    def load_best_model(self):
        model_path = f"{self.save_dir}/{self.model_name}_best_model.pth"
        self.model.load_state_dict(torch.load(model_path))
        self.model.eval()
        print(f"Loaded best model from {model_path}")

"""##TESTING AND EVALUTION"""

class ModelEvaluator:

    def __init__(self, model, model_name, device, class_names):
        self.model = model
        self.model_name = model_name
        self.device = device
        self.class_names = class_names
        self.num_classes = len(class_names)

    def to_device(self, batch):
        return batch[0].to(self.device), batch[1].to(self.device)

    def evaluate(self, test_loader):
        """Comprehensive evaluation on test set"""
        self.model.eval()

        all_preds = []
        all_labels = []
        all_probs = []
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in tqdm(test_loader, desc=f"Testing {self.model_name}"):
                ims, gts = self.to_device(batch)

                outputs = self.model(ims)
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(gts.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())

                correct += (preds == gts).sum().item()
                total += gts.size(0)

        accuracy = correct / total

        # Calculate per-class metrics
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)

        return {
            'accuracy': accuracy,
            'predictions': all_preds,
            'labels': all_labels,
            'probabilities': np.array(all_probs)
        }

    def calculate_detailed_metrics(self, results):
        """Calculate precision, recall, F1 for each class"""
        from sklearn.metrics import classification_report, confusion_matrix

        predictions = results['predictions']
        labels = results['labels']

        # Classification report
        report = classification_report(
            labels,
            predictions,
            target_names=self.class_names,
            output_dict=True,
            zero_division=0
        )

        # Confusion matrix
        conf_matrix = confusion_matrix(labels, predictions)

        return report, conf_matrix

    def print_results(self, results, report):
        """Print evaluation results"""
        print(f"\n{'='*60}")
        print(f"Test Results for {self.model_name}")
        print(f"{'='*60}")
        print(f"Overall Accuracy: {results['accuracy']:.4f}")
        print(f"Macro Avg Precision: {report['macro avg']['precision']:.4f}")
        print(f"Macro Avg Recall: {report['macro avg']['recall']:.4f}")
        print(f"Macro Avg F1-Score: {report['macro avg']['f1-score']:.4f}")
        print(f"Weighted Avg F1-Score: {report['weighted avg']['f1-score']:.4f}")
        print(f"{'='*60}\n")

"""##VIZUALTION CHANGES"""

class PlotLearningCurves:

    def __init__(self, history, model_name):
        self.history = history
        self.model_name = model_name

    def plot(self, array_1, array_2, label_1, label_2, color_1, color_2):
        plt.plot(array_1, label=label_1, c=color_1, marker='o')
        plt.plot(array_2, label=label_2, c=color_2, marker='s')

    def create_figure(self):
        plt.figure(figsize=(10, 6))

    def decorate(self, ylabel, xlabel="Epochs"):
        plt.xlabel(xlabel, fontsize=12)
        plt.ylabel(ylabel, fontsize=12)
        epochs = len(self.history['train_loss'])
        plt.xticks(ticks=np.arange(epochs), labels=[i+1 for i in range(epochs)])
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    def visualize(self):

        # Loss plot
        self.create_figure()
        self.plot(
            array_1=self.history['train_loss'],
            array_2=self.history['val_loss'],
            label_1="Train Loss",
            label_2="Validation Loss",
            color_1="red",
            color_2="blue"
        )
        plt.title(f"{self.model_name} - Loss Curves", fontsize=14)
        self.decorate(ylabel="Loss Values")

        # Accuracy plot
        self.create_figure()
        self.plot(
            array_1=self.history['train_acc'],
            array_2=self.history['val_acc'],
            label_1="Train Accuracy",
            label_2="Validation Accuracy",
            color_1="orangered",
            color_2="darkgreen"
        )
        plt.title(f"{self.model_name} - Accuracy Curves", fontsize=14)
        self.decorate(ylabel="Accuracy Scores")

        # F1 Score plot
        self.create_figure()
        self.plot(
            array_1=self.history['train_f1'],
            array_2=self.history['val_f1'],
            label_1="Train F1 Score",
            label_2="Validation F1 Score",
            color_1="blueviolet",
            color_2="crimson"
        )
        plt.title(f"{self.model_name} - F1 Score Curves", fontsize=14)
        self.decorate(ylabel="F1 Scores")


class ConfusionMatrixPlotter:

    def __init__(self, conf_matrix, class_names, model_name):
        self.conf_matrix = conf_matrix
        self.class_names = class_names
        self.model_name = model_name

    def plot(self, normalize=False):
        """Plot confusion matrix"""
        import seaborn as sns

        if normalize:
            conf_matrix = self.conf_matrix.astype('float') / self.conf_matrix.sum(axis=1)[:, np.newaxis]
            fmt = '.2f'
            title = f'{self.model_name} - Normalized Confusion Matrix'
        else:
            conf_matrix = self.conf_matrix
            fmt = 'd'
            title = f'{self.model_name} - Confusion Matrix'

        plt.figure(figsize=(12, 10))
        sns.heatmap(
            conf_matrix,
            annot=True,
            fmt=fmt,
            cmap='Blues',
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            cbar_kws={'label': 'Count' if not normalize else 'Proportion'}
        )
        plt.title(title, fontsize=14)
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.show()

"""GRAD CAM CHANGES"""

class SaveFeatures():
    """Extract pretrained activations"""
    features = None

    def __init__(self, m):
        self.hook = m.register_forward_hook(self.hook_fn)

    def hook_fn(self, module, input, output):
        self.features = ((output.cpu()).data).numpy()

    def remove(self):
        self.hook.remove()


def getCAM(conv_fs, linear_weights, class_idx):
    """Generate Class Activation Map"""
    bs, chs, h, w = conv_fs.shape
    cam = linear_weights[class_idx].dot(conv_fs[0, :, :, ].reshape((chs, h * w)))
    cam = cam.reshape(h, w)

    cam = cam - np.min(cam)
    cam_max = np.max(cam)
    if cam_max != 0:
        cam = cam / cam_max

    return cam


class GradCAMVisualizer:

    def __init__(self, model, model_name, device, class_names, im_size=224):
        self.model = model
        self.model_name = model_name
        self.device = device
        self.class_names = class_names
        self.im_size = im_size

    def get_target_layer(self):
        """Get the last convolutional layer for different architectures"""

        # For ViT models
        if 'vit' in self.model_name.lower():
            # ViT doesn't have traditional conv layers, use last block
            return self.model.blocks[-1].norm1

        # For ConvNeXt models
        elif 'convnext' in self.model_name.lower():
            return self.model.stages[-1].blocks[-1]

        # For EfficientNet models
        elif 'efficientnet' in self.model_name.lower():
            return self.model.conv_head

        else:
            # Default: try to find last conv layer
            for name, module in reversed(list(self.model.named_modules())):
                if isinstance(module, torch.nn.Conv2d):
                    return module
            return None

    def get_fc_weights(self):
        """Get final classifier weights"""

        # For ViT
        if hasattr(self.model, 'head'):
            if hasattr(self.model.head, 'weight'):
                return self.model.head.weight
            elif hasattr(self.model.head, 'fc'):
                return self.model.head.fc.weight

        # For other models
        if hasattr(self.model, 'classifier'):
            return self.model.classifier.weight

        # For models with 'fc' or 'head'
        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.Linear):
                return module.weight

        return None

    def to_device(self, batch):
        return batch[0].to(self.device), batch[1].to(self.device)

    def visualize(self, test_dl, num_ims=20, row=4):
        """Visualize predictions with GradCAM"""

        self.model.eval()

        target_layer = self.get_target_layer()
        fc_weights = self.get_fc_weights()

        if target_layer is None or fc_weights is None:
            print(f"Warning: Could not extract layers for GradCAM visualization for {self.model_name}")
            print("Showing predictions without heatmaps...")
            self._visualize_without_gradcam(test_dl, num_ims, row)
            return

        activated_features = SaveFeatures(target_layer)
        weight = fc_weights.cpu().data.numpy()

        preds, images, lbls = [], [], []
        correct = 0

        with torch.no_grad():
            for idx, batch in tqdm(enumerate(test_dl), desc="Generating visualizations"):
                im, gt = self.to_device(batch)
                pred_class = torch.argmax(self.model(im), dim=1)
                correct += (pred_class == gt).sum().item()

                images.append(im)
                preds.append(pred_class.item())
                lbls.append(gt.item())

        print(f"\nAccuracy on test set: {(correct / len(test_dl.dataset)):.4f}")

        # Plot
        plt.figure(figsize=(20, 10))
        indices = [random.randint(0, len(images) - 1) for _ in range(num_ims)]

        for idx, indeks in enumerate(indices):

            im = images[indeks].squeeze()
            pred_idx = preds[indeks]
            true_idx = lbls[indeks]

            # Generate heatmap if features are available
            if activated_features.features is not None and activated_features.features.ndim == 4:
                heatmap = getCAM(activated_features.features, weight, pred_idx)
            else:
                heatmap = None

            # Plot
            plt.subplot(row, num_ims // row, idx + 1)
            plt.imshow(tensor_2_im(im))
            plt.axis("off")

            if heatmap is not None:
                plt.imshow(
                    cv2.resize(heatmap, (self.im_size, self.im_size),
                              interpolation=cv2.INTER_LINEAR),
                    alpha=0.4,
                    cmap='jet'
                )

            title_color = "green" if pred_idx == true_idx else "red"
            plt.title(
                f"GT: {self.class_names[true_idx]}\nPred: {self.class_names[pred_idx]}",
                color=title_color,
                fontsize=10
            )

        plt.suptitle(f'{self.model_name} - Predictions with GradCAM', fontsize=16)
        plt.tight_layout()
        plt.show()

        activated_features.remove()

    def _visualize_without_gradcam(self, test_dl, num_ims, row):
        """Fallback visualization without GradCAM"""

        preds, images, lbls = [], [], []
        correct = 0

        with torch.no_grad():
            for batch in tqdm(test_dl, desc="Generating predictions"):
                im, gt = self.to_device(batch)
                pred_class = torch.argmax(self.model(im), dim=1)
                correct += (pred_class == gt).sum().item()

                images.append(im)
                preds.append(pred_class.item())
                lbls.append(gt.item())

        print(f"\nAccuracy on test set: {(correct / len(test_dl.dataset)):.4f}")

        plt.figure(figsize=(20, 10))
        indices = [random.randint(0, len(images) - 1) for _ in range(num_ims)]

        for idx, indeks in enumerate(indices):

            im = images[indeks].squeeze()
            pred_idx = preds[indeks]
            true_idx = lbls[indeks]

            plt.subplot(row, num_ims // row, idx + 1)
            plt.imshow(tensor_2_im(im))
            plt.axis("off")

            title_color = "green" if pred_idx == true_idx else "red"
            plt.title(
                f"GT: {self.class_names[true_idx]}\nPred: {self.class_names[pred_idx]}",
                color=title_color,
                fontsize=10
            )

        plt.suptitle(f'{self.model_name} - Predictions', fontsize=16)
        plt.tight_layout()
        plt.show()

"""##Main Training and Comparison Pipeline"""

class ModelComparison:

    def __init__(self, model_configs, num_classes, class_names, device='cuda'):
        self.model_configs = model_configs
        self.num_classes = num_classes
        self.class_names = class_names
        self.device = device
        self.results = {}

    def create_model(self, config):
        """Create model from config"""
        model = timm.create_model(
            config['model_name'],
            pretrained=config['pretrained'],
            num_classes=self.num_classes
        )
        return model

    def train_single_model(self, model_key, train_loader, val_loader, epochs=10, patience=5):
        """Train a single model"""

        print(f"\n{'#'*80}")
        print(f"# Training {model_key}: {self.model_configs[model_key]['description']}")
        print(f"{'#'*80}\n")

        # Create model
        model = self.create_model(self.model_configs[model_key])

        # Create trainer
        trainer = ModelTrainer(
            model_name=model_key,
            model=model,
            device=self.device,
            num_classes=self.num_classes,
            save_dir="saved_models"
        )

        # Train
        history = trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=epochs,
            patience=patience
        )

        # Store results
        self.results[model_key] = {
            'trainer': trainer,
            'history': history,
            'model': trainer.model
        }

        return history

    def train_all_models(self, train_loader, val_loader, epochs=10, patience=5):
        """Train all models in the configuration"""

        print("\n" + "="*80)
        print("STARTING TRAINING FOR ALL MODELS")
        print("="*80)

        for model_key in self.model_configs.keys():
            try:
                self.train_single_model(
                    model_key=model_key,
                    train_loader=train_loader,
                    val_loader=val_loader,
                    epochs=epochs,
                    patience=patience
                )

                # Clear cache
                torch.cuda.empty_cache()

            except Exception as e:
                print(f"\n❌ Error training {model_key}: {str(e)}")
                continue

        print("\n" + "="*80)
        print("TRAINING COMPLETED FOR ALL MODELS")
        print("="*80 + "\n")

    def evaluate_all_models(self, test_loader):
        """Evaluate all trained models"""

        print("\n" + "="*80)
        print("EVALUATING ALL MODELS")
        print("="*80 + "\n")

        evaluation_results = {}

        for model_key in self.results.keys():

            print(f"\nEvaluating {model_key}...")

            try:
                # Load best model
                trainer = self.results[model_key]['trainer']
                trainer.load_best_model()

                # Create evaluator
                evaluator = ModelEvaluator(
                    model=trainer.model,
                    model_name=model_key,
                    device=self.device,
                    class_names=self.class_names
                )

                # Evaluate
                results = evaluator.evaluate(test_loader)
                report, conf_matrix = evaluator.calculate_detailed_metrics(results)
                evaluator.print_results(results, report)

                # Store results
                evaluation_results[model_key] = {
                    'evaluator': evaluator,
                    'results': results,
                    'report': report,
                    'conf_matrix': conf_matrix
                }

                # Clear cache
                torch.cuda.empty_cache()

            except Exception as e:
                print(f"\n❌ Error evaluating {model_key}: {str(e)}")
                continue

        self.evaluation_results = evaluation_results

        print("\n" + "="*80)
        print("EVALUATION COMPLETED FOR ALL MODELS")
        print("="*80 + "\n")

        return evaluation_results

    def visualize_all_learning_curves(self):
        """Visualize learning curves for all models"""

        for model_key, data in self.results.items():
            print(f"\nLearning curves for {model_key}:")
            plotter = PlotLearningCurves(data['history'], model_key)
            plotter.visualize()

    def visualize_all_confusion_matrices(self, normalize=False):
        """Visualize confusion matrices for all models"""

        if not hasattr(self, 'evaluation_results'):
            print("Please run evaluate_all_models() first!")
            return

        for model_key, data in self.evaluation_results.items():
            print(f"\nConfusion matrix for {model_key}:")
            plotter = ConfusionMatrixPlotter(
                conf_matrix=data['conf_matrix'],
                class_names=self.class_names,
                model_name=model_key
            )
            plotter.plot(normalize=normalize)

    def visualize_all_gradcam(self, test_loader, num_ims=20, row=4):
        """Generate GradCAM visualizations for all models"""

        if not hasattr(self, 'evaluation_results'):
            print("Please run evaluate_all_models() first!")
            return

        for model_key in self.results.keys():

            print(f"\nGenerating GradCAM for {model_key}...")

            try:
                trainer = self.results[model_key]['trainer']
                trainer.load_best_model()

                visualizer = GradCAMVisualizer(
                    model=trainer.model,
                    model_name=model_key,
                    device=self.device,
                    class_names=self.class_names,
                    im_size=im_size
                )

                visualizer.visualize(test_loader, num_ims=num_ims, row=row)

            except Exception as e:
                print(f"❌ Error generating GradCAM for {model_key}: {str(e)}")
                continue

    def generate_comparison_table(self):
        """Generate a comparison table of all models"""

        if not hasattr(self, 'evaluation_results'):
            print("Please run evaluate_all_models() first!")
            return None

        comparison_data = []

        for model_key, data in self.evaluation_results.items():
            report = data['report']
            results = data['results']

            comparison_data.append({
                'Model': model_key,
                'Accuracy': f"{results['accuracy']:.4f}",
                'Precision (Macro)': f"{report['macro avg']['precision']:.4f}",
                'Recall (Macro)': f"{report['macro avg']['recall']:.4f}",
                'F1-Score (Macro)': f"{report['macro avg']['f1-score']:.4f}",
                'F1-Score (Weighted)': f"{report['weighted avg']['f1-score']:.4f}"
            })

        df = pd.DataFrame(comparison_data)

        print("\n" + "="*80)
        print("MODEL COMPARISON TABLE")
        print("="*80 + "\n")
        print(df.to_string(index=False))
        print("\n" + "="*80 + "\n")

        # Save to CSV
        df.to_csv('saved_models/model_comparison.csv', index=False)
        print("Comparison table saved to 'saved_models/model_comparison.csv'")

        return df

    def plot_model_comparison(self):
        """Plot comparison charts for all models"""

        if not hasattr(self, 'evaluation_results'):
            print("Please run evaluate_all_models() first!")
            return

        models = []
        accuracies = []
        precisions = []
        recalls = []
        f1_scores = []

        for model_key, data in self.evaluation_results.items():
            report = data['report']
            results = data['results']

            models.append(model_key)
            accuracies.append(results['accuracy'])
            precisions.append(report['macro avg']['precision'])
            recalls.append(report['macro avg']['recall'])
            f1_scores.append(report['macro avg']['f1-score'])

        # Create subplots
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # Accuracy comparison
        axes[0, 0].bar(models, accuracies, color='steelblue')
        axes[0, 0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')
        axes[0, 0].set_ylabel('Accuracy', fontsize=12)
        axes[0, 0].set_ylim([0, 1])
        axes[0, 0].grid(axis='y', alpha=0.3)
        for i, v in enumerate(accuracies):
            axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')

        # Precision comparison
        axes[0, 1].bar(models, precisions, color='coral')
        axes[0, 1].set_title('Precision Comparison (Macro Avg)', fontsize=14, fontweight='bold')
        axes[0, 1].set_ylabel('Precision', fontsize=12)
        axes[0, 1].set_ylim([0, 1])
        axes[0, 1].grid(axis='y', alpha=0.3)
        for i, v in enumerate(precisions):
            axes[0, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')

        # Recall comparison
        axes[1, 0].bar(models, recalls, color='mediumseagreen')
        axes[1, 0].set_title('Recall Comparison (Macro Avg)', fontsize=14, fontweight='bold')
        axes[1, 0].set_ylabel('Recall', fontsize=12)
        axes[1, 0].set_ylim([0, 1])
        axes[1, 0].grid(axis='y', alpha=0.3)
        for i, v in enumerate(recalls):
            axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')

        # F1-Score comparison
        axes[1, 1].bar(models, f1_scores, color='mediumpurple')
        axes[1, 1].set_title('F1-Score Comparison (Macro Avg)', fontsize=14, fontweight='bold')
        axes[1, 1].set_ylabel('F1-Score', fontsize=12)
        axes[1, 1].set_ylim([0, 1])
        axes[1, 1].grid(axis='y', alpha=0.3)
        for i, v in enumerate(f1_scores):
            axes[1, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')

        # Rotate x labels
        for ax in axes.flat:
            ax.tick_params(axis='x', rotation=45)

        plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig('saved_models/model_comparison_plot.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("Comparison plot saved to 'saved_models/model_comparison_plot.png'")

    def generate_detailed_report(self, output_file='saved_models/detailed_report.txt'):
        """Generate detailed text report"""

        if not hasattr(self, 'evaluation_results'):
            print("Please run evaluate_all_models() first!")
            return

        with open(output_file, 'w') as f:
            f.write("="*80 + "\n")
            f.write("CROP DISEASE DETECTION - MODEL COMPARISON REPORT\n")
            f.write("="*80 + "\n\n")
            f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Number of classes: {self.num_classes}\n")
            f.write(f"Classes: {', '.join(self.class_names)}\n\n")

            for model_key, data in self.evaluation_results.items():
                f.write("\n" + "="*80 + "\n")
                f.write(f"Model: {model_key}\n")
                f.write(f"Description: {self.model_configs[model_key]['description']}\n")
                f.write("="*80 + "\n\n")

                report = data['report']
                results = data['results']

                f.write("Overall Metrics:\n")
                f.write("-" * 40 + "\n")
                f.write(f"Accuracy: {results['accuracy']:.4f}\n")
                f.write(f"Macro Avg Precision: {report['macro avg']['precision']:.4f}\n")
                f.write(f"Macro Avg Recall: {report['macro avg']['recall']:.4f}\n")
                f.write(f"Macro Avg F1-Score: {report['macro avg']['f1-score']:.4f}\n")
                f.write(f"Weighted Avg F1-Score: {report['weighted avg']['f1-score']:.4f}\n\n")

                f.write("Per-Class Metrics:\n")
                f.write("-" * 40 + "\n")
                f.write(f"{'Class':<25} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\n")
                f.write("-" * 40 + "\n")

                for class_name in self.class_names:
                    if class_name in report:
                        f.write(f"{class_name:<25} "
                               f"{report[class_name]['precision']:<12.4f} "
                               f"{report[class_name]['recall']:<12.4f} "
                               f"{report[class_name]['f1-score']:<12.4f} "
                               f"{int(report[class_name]['support']):<12}\n")

                f.write("\n")

        print(f"Detailed report saved to '{output_file}'")

"""##COMPLET EXECUTION PIPELINE"""

# ========================================
# COMPLETE EXECUTION PIPELINE
# ========================================

def run_complete_experiment(
    model_configs=MODEL_CONFIGS,
    train_dl=tr_dl,
    val_dl=val_dl,
    test_dl=ts_dl,
    classes=classes,
    epochs=10,
    patience=5,
    device='cuda'
):
    """
    Run the complete experimental pipeline
    """

    print("\n" + "#"*80)
    print("# CROP DISEASE DETECTION - COMPREHENSIVE MODEL COMPARISON")
    print("#"*80 + "\n")

    # Initialize comparison
    comparison = ModelComparison(
        model_configs=model_configs,
        num_classes=len(classes),
        class_names=list(classes.keys()),
        device=device
    )

    # Step 1: Train all models
    print("\n" + "▶"*40)
    print("STEP 1: TRAINING ALL MODELS")
    print("▶"*40 + "\n")
    comparison.train_all_models(
        train_loader=train_dl,
        val_loader=val_dl,
        epochs=epochs,
        patience=patience
    )

    # Step 2: Visualize
    # Step 2: Visualize learning curves
    print("\n" + "▶"*40)
    print("STEP 2: VISUALIZING LEARNING CURVES")
    print("▶"*40 + "\n")
    comparison.visualize_all_learning_curves()

    # Step 3: Evaluate all models
    print("\n" + "▶"*40)
    print("STEP 3: EVALUATING ALL MODELS ON TEST SET")
    print("▶"*40 + "\n")
    comparison.evaluate_all_models(test_loader=test_dl)

    # Step 4: Generate comparison table
    print("\n" + "▶"*40)
    print("STEP 4: GENERATING COMPARISON TABLE")
    print("▶"*40 + "\n")
    comparison_df = comparison.generate_comparison_table()

    # Step 5: Plot comparison charts
    print("\n" + "▶"*40)
    print("STEP 5: GENERATING COMPARISON PLOTS")
    print("▶"*40 + "\n")
    comparison.plot_model_comparison()

    # Step 6: Visualize confusion matrices
    print("\n" + "▶"*40)
    print("STEP 6: VISUALIZING CONFUSION MATRICES")
    print("▶"*40 + "\n")
    comparison.visualize_all_confusion_matrices(normalize=False)
    comparison.visualize_all_confusion_matrices(normalize=True)

    # Step 7: Generate GradCAM visualizations
    print("\n" + "▶"*40)
    print("STEP 7: GENERATING GRADCAM VISUALIZATIONS")
    print("▶"*40 + "\n")
    comparison.visualize_all_gradcam(test_loader=test_dl, num_ims=20, row=4)

    # Step 8: Generate detailed report
    print("\n" + "▶"*40)
    print("STEP 8: GENERATING DETAILED REPORT")
    print("▶"*40 + "\n")
    comparison.generate_detailed_report()

    print("\n" + "#"*80)
    print("# EXPERIMENT COMPLETED SUCCESSFULLY!")
    print("#"*80 + "\n")

    return comparison, comparison_df


# ========================================
# RUN THE COMPLETE EXPERIMENT
# ========================================

# Execute the complete pipeline
comparison, results_df = run_complete_experiment(
    model_configs=MODEL_CONFIGS,
    train_dl=tr_dl,
    val_dl=val_dl,
    test_dl=ts_dl,
    classes=classes,
    epochs=4,  # Adjust as needed
    patience=5,
    device='cuda' if torch.cuda.is_available() else 'cpu'
)